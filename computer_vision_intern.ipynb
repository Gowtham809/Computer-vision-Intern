{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Basketball Dribble Counter\n",
    "#Method 1 \n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Read the video\n",
    "cap = cv2.VideoCapture('C:/Users/Admin/Downloads/WHATSAAP ASSIGNMENT.mp4')\n",
    "\n",
    "# Variables for touch count\n",
    "touch_count = 0\n",
    "is_touched = False\n",
    "distance_sequence = []\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the lower and upper bounds for yellow color\n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([30, 255, 255])\n",
    "\n",
    "    # Create a mask to isolate yellow regions\n",
    "    mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise and help the Hough Circle Transform\n",
    "    blurred = cv2.GaussianBlur(mask, (11, 11), 0)\n",
    "\n",
    "    # Use Hough Circle Transform to detect circles\n",
    "    circles = cv2.HoughCircles(\n",
    "        blurred,\n",
    "        cv2.HOUGH_GRADIENT,\n",
    "        dp=1,\n",
    "        minDist=40,\n",
    "        param1=50,\n",
    "        param2=20,\n",
    "        minRadius=10,\n",
    "        maxRadius=50\n",
    "    )\n",
    "\n",
    "    if circles is not None:\n",
    "        # Convert the (x, y) coordinates and radius of the circles to integers\n",
    "        circles = np.round(circles[0, :]).astype(\"int\")\n",
    "\n",
    "        for (x, y, r) in circles:\n",
    "            # Draw the circle in the output image\n",
    "            cv2.circle(frame, (x, y), r, (0, 255, 0), 4)\n",
    "\n",
    "            # Process the frame with Mediapipe hands\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(rgb_frame)\n",
    "\n",
    "            # Check if hands are detected\n",
    "            if results.multi_hand_landmarks and len(results.multi_hand_landmarks) >= 2:\n",
    "                # Get the middle finger landmarks for both hands\n",
    "                left_hand_landmarks = results.multi_hand_landmarks[0]\n",
    "                right_hand_landmarks = results.multi_hand_landmarks[1]\n",
    "\n",
    "                # Get the middle finger landmark coordinates\n",
    "                left_middle_finger = left_hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                right_middle_finger = right_hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "\n",
    "                # Convert normalized coordinates to pixel coordinates\n",
    "                height, width, _ = frame.shape\n",
    "                left_middle_finger_x, left_middle_finger_y = int(left_middle_finger.x * width), int(left_middle_finger.y * height)\n",
    "                right_middle_finger_x, right_middle_finger_y = int(right_middle_finger.x * width), int(right_middle_finger.y * height)\n",
    "\n",
    "                # Calculate distance between the hands and the circle center\n",
    "                distance_left = np.sqrt((left_middle_finger_x - x)**2 + (left_middle_finger_y - y)**2)\n",
    "                distance_right = np.sqrt((right_middle_finger_x - x)**2 + (right_middle_finger_y - y)**2)\n",
    "\n",
    "                # Display distance on the frame with resized font\n",
    "                font_size = 0.8\n",
    "                cv2.putText(frame, f'Distance to Left Hand: {distance_left:.2f}', (50, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, font_size, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                cv2.putText(frame, f'Distance to Right Hand: {distance_right:.2f}', (50, 80),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, font_size, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Find the minimum distance\n",
    "                min_distance = min(distance_left, distance_right)\n",
    "\n",
    "                # Update the distance sequence\n",
    "                distance_sequence.append(min_distance)\n",
    "\n",
    "                # Display the minimum distance on the frame\n",
    "                cv2.putText(frame, f'Minimum Distance: {min_distance:.2f}', (50, 110),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, font_size, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Check if the hand and circle touch\n",
    "                if min_distance < r:\n",
    "                    if not is_touched:\n",
    "                        is_touched = True\n",
    "                else:\n",
    "                    is_touched = False\n",
    "\n",
    "                # Check if the sequence of minimum distances is less than 100\n",
    "                if len(distance_sequence) >= 3 and all(d < 100 for d in distance_sequence[-3:]):\n",
    "                    # Increase the touch count\n",
    "                    touch_count += 1\n",
    "                    # Reset the distance sequence\n",
    "                    distance_sequence = []\n",
    "\n",
    "        # Display touch count on the frame with resized font\n",
    "        cv2.putText(frame, f'Touch Count: {touch_count}', (50, 150),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(85) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#work_ball count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ball coordinates: (x=212.49, y=245.45)\n",
      "Ball coordinates: (x=211.68, y=263.43)\n",
      "Ball coordinates: (x=207.34, y=320.41)\n",
      "Ball coordinates: (x=184.71, y=74.64)\n",
      "Ball coordinates: (x=184.07, y=74.81)\n",
      "Ball coordinates: (x=183.42, y=74.94)\n",
      "Ball coordinates: (x=191.43, y=559.12)\n",
      "Ball coordinates: (x=211.80, y=319.88)\n",
      "Ball coordinates: (x=214.81, y=302.55)\n",
      "Ball coordinates: (x=217.17, y=300.61)\n",
      "Ball coordinates: (x=217.36, y=309.83)\n",
      "Ball coordinates: (x=217.29, y=329.39)\n"
     ]
    }
   ],
   "source": [
    "#Basketball Dribble Counter\n",
    "#Method 2\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "class DribbleCounter:\n",
    "    def __init__(self, video_path):\n",
    "        # Load the YOLO model for ball detection\n",
    "        self.model = YOLO(\"C:/Users/Admin/Downloads/basketballModel.pt\")\n",
    "\n",
    "        # Open the video file\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Initialize variables to store the previous position of the basketball\n",
    "        self.prev_x_center = None\n",
    "        self.prev_y_center = None\n",
    "        self.prev_delta_y = None\n",
    "\n",
    "        # Initialize the dribble counter\n",
    "        self.dribble_count = 0\n",
    "\n",
    "        # Threshold for the y-coordinate change to be considered as a dribble\n",
    "        self.dribble_threshold = 10\n",
    "\n",
    "    def run(self):\n",
    "        # Process frames from the video until the end\n",
    "        while self.cap.isOpened():\n",
    "            success, frame = self.cap.read()\n",
    "            if success:\n",
    "                self.process_frame(frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Release the video capture and destroy the window\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        results_list = self.model(frame, verbose=False, conf=0.50)\n",
    "\n",
    "        for results in results_list:\n",
    "            for bbox in results.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = bbox[:4]\n",
    "\n",
    "                x_center = (x1 + x2) / 2\n",
    "                y_center = (y1 + y2) / 2\n",
    "\n",
    "                print(f\"Ball coordinates: (x={x_center:.2f}, y={y_center:.2f})\")\n",
    "\n",
    "                self.update_dribble_count(x_center, y_center)\n",
    "\n",
    "                self.prev_x_center = x_center\n",
    "                self.prev_y_center = y_center\n",
    "\n",
    "            annotated_frame = results.plot()\n",
    "\n",
    "            # Draw the dribble count on the frame\n",
    "            cv2.putText(annotated_frame, f'Dribble Count: {self.dribble_count}', (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "    def update_dribble_count(self, x_center, y_center):\n",
    "        if self.prev_y_center is not None:\n",
    "            delta_y = y_center - self.prev_y_center\n",
    "\n",
    "            if (\n",
    "                self.prev_delta_y is not None\n",
    "                and self.prev_delta_y > self.dribble_threshold\n",
    "                and delta_y < -self.dribble_threshold\n",
    "            ):\n",
    "                self.dribble_count += 1\n",
    "\n",
    "            self.prev_delta_y = delta_y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"C:/Users/Admin/Downloads/WHATSAAP ASSIGNMENT.mp4\"\n",
    "    dribble_counter = DribbleCounter(video_path)\n",
    "    dribble_counter.run()\n",
    "#10-97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trajectory of the Ball \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Replace 'your_video_path.mp4' with the actual path to your video file\n",
    "video_path = 'C:/Users/Admin/Downloads/WHATSAAP ASSIGNMENT.mp4'\n",
    "\n",
    "# If a video path was not supplied, use the webcam\n",
    "if not video_path:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Set the desired frame size for visibility on the laptop screen\n",
    "visible_width = 640\n",
    "visible_height = 480\n",
    "\n",
    "# Set the frame width and height\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, visible_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, visible_height)\n",
    "\n",
    "# Initialize the video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_path = 'output_trajectory.avi'\n",
    "out = cv2.VideoWriter(output_path, fourcc, 20.0, (visible_width, visible_height))\n",
    "\n",
    "# Define color range for the basketball\n",
    "colourLow = (0, 100, 50)\n",
    "colourHigh = (20, 255, 255)\n",
    "\n",
    "# Initialize variables for trajectory detection\n",
    "trajectory_points = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    # Resize the frame to the desired visible size\n",
    "    frame = cv2.resize(frame, (visible_width, visible_height))\n",
    "\n",
    "    blurred = cv2.GaussianBlur(frame, (11, 11), 0)\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    mask = cv2.inRange(hsv, colourLow, colourHigh)\n",
    "    mask = cv2.erode(mask, None, iterations=2)\n",
    "    mask = cv2.dilate(mask, None, iterations=2)\n",
    "\n",
    "    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    center = None\n",
    "\n",
    "    if len(cnts) > 0:\n",
    "        c = max(cnts, key=cv2.contourArea)\n",
    "        ((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "        M = cv2.moments(c)\n",
    "        center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "\n",
    "        if radius > 15 and cv2.contourArea(c) > 150:\n",
    "            cv2.circle(frame, (int(x), int(y)), int(radius), (0, 255, 255), 2)\n",
    "            cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "\n",
    "            # Add the current center to the trajectory points\n",
    "            trajectory_points.append(center)\n",
    "\n",
    "            # Draw the trajectory on the frame\n",
    "            for i in range(1, len(trajectory_points)):\n",
    "                cv2.line(frame, trajectory_points[i - 1], trajectory_points[i], (0, 255, 0), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "#work_1_trajectory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optical Flow of Objects \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Replace 'your_video_path.mp4' with the actual path to your video file\n",
    "video_path = 'C:/Users/Admin/Downloads/WHATSAAP ASSIGNMENT.mp4'\n",
    "\n",
    "# If a video path was not supplied, use the webcam\n",
    "if not video_path:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Take the first frame\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "# Define a random set of points in the first frame\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(\n",
    "        old_gray, frame_gray, p0, None, winSize=(15, 15), maxLevel=2,\n",
    "        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "    )\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p0[st == 1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel().astype(int)\n",
    "        c, d = old.ravel().astype(int)\n",
    "        mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)\n",
    "        frame = cv2.circle(frame, (a, b), 5, (0, 0, 255), -1)\n",
    "\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    cv2.imshow('Optical Flow', img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "    # Update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#worker_opticalflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatically Detecting Outlines And Borders of Objects\n",
    "import os\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Video Processor\")\n",
    "\n",
    "        self.instructions = [\n",
    "            \"1. After clicking 'Run Video Processing,' press 's' to pause the video.\",\n",
    "            \"2. After pressing 's,' use the mouse to click and crop the image, then press 'Enter.'\",\n",
    "            \"3. After pressing 'Enter,' you can view the cropped image.\",\n",
    "            \"4. After viewing the cropped image, press 'c' to clear and repeat the process of clicking and cropping the image to view the cropped image again.\",\n",
    "            \"5. Only when you are viewing the cropped image can you quit the repeating process by pressing 'q.'\",\n",
    "            \"6. After pressing 'q,' you will return to resume the video that was paused, and you can repeat the process.\"\n",
    "        ]\n",
    "\n",
    "        self.instructions_label = tk.Label(root, text=\"\\n\".join(self.instructions), justify='left')\n",
    "        self.instructions_label.pack()\n",
    "\n",
    "        self.run_button = tk.Button(root, text=\"Run Video Processing\", command=self.run_video_processing)\n",
    "        self.run_button.pack()\n",
    "\n",
    "        self.exit_button = tk.Button(root, text=\"Exit\", command=self.root.destroy)\n",
    "        self.exit_button.pack()\n",
    "\n",
    "    def run_video_processing(self):\n",
    "        messagebox.showinfo(\"Video Processor\", \"Video processing started. Press 's' to pause the video.\")\n",
    "        self.root.update_idletasks()  # Update the Tkinter window\n",
    "\n",
    "        input_video_path = \"C:/Users/Admin/Downloads/WHATSAAP ASSIGNMENT.mp4\"\n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            messagebox.showerror(\"Video Processor\", \"Error opening video\")\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                messagebox.showerror(\"Video Processor\", \"Video frame completed \")\n",
    "                break\n",
    "\n",
    "            cv2.imshow('Video Viewer', frame)\n",
    "            self.root.update_idletasks()  # Update the Tkinter window\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            if key == ord('s'):\n",
    "                self.user_interactions(frame, cap)\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def user_interactions(self, img, cap):\n",
    "        original_img = img.copy()\n",
    "\n",
    "        while True:\n",
    "            roi = cv2.selectROI(img)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            x, y, w, h = roi\n",
    "\n",
    "            if x < 0 or y < 0 or x + w > img.shape[1] or y + h > img.shape[0]:\n",
    "                print(\"Invalid ROI. Please try again.\")\n",
    "                img = original_img.copy()\n",
    "                continue\n",
    "\n",
    "            cropped_img = img[y:y+h, x:x+w]\n",
    "\n",
    "            # Perform image processing techniques (resize, brighten, etc.)\n",
    "            processed_img = self.process_cropped_image(cropped_img)\n",
    "\n",
    "            # Display the processed image\n",
    "            cv2.imshow('Processed Cropped Image', processed_img)\n",
    "\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('c'):\n",
    "                img = original_img.copy()\n",
    "\n",
    "            # Save the processed image with a random number in the name\n",
    "            save_dir = 'E:/computer_vision_intern/images/'\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "\n",
    "            try:\n",
    "                img_name = f'processed_cropped_image_{random.randint(1, 100000)}.jpg'\n",
    "                full_path = os.path.join(save_dir, img_name)\n",
    "                cv2.imwrite(full_path, processed_img)\n",
    "\n",
    "                print(f\"Saved image: {full_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image: {str(e)}\")\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def process_cropped_image(self, img):\n",
    "        # Resize the image to (216, 216)\n",
    "        resized_img = cv2.resize(img, (416, 416))\n",
    "\n",
    "        # Brighten the image\n",
    "        brightness_factor = 1.5\n",
    "        brightened_img = cv2.convertScaleAbs(resized_img, alpha=brightness_factor, beta=0)\n",
    "\n",
    "        return brightened_img\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    video_processor = VideoProcessor(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
